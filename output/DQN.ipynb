{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1fede552",
   "metadata": {},
   "source": [
    "<img src=\"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAG8AAABuCAYAAAApmU3FAAAEtmlUWHRYTUw6Y29tLmFkb2JlLnht\n",
    "cAAAAAAAPD94cGFja2V0IGJlZ2luPSLvu78iIGlkPSJXNU0wTXBDZWhpSHpyZVN6TlRjemtjOWQi\n",
    "Pz4KPHg6eG1wbWV0YSB4bWxuczp4PSJhZG9iZTpuczptZXRhLyIgeDp4bXB0az0iWE1QIENvcmUg\n",
    "NS41LjAiPgogPHJkZjpSREYgeG1sbnM6cmRmPSJodHRwOi8vd3d3LnczLm9yZy8xOTk5LzAyLzIy\n",
    "LXJkZi1zeW50YXgtbnMjIj4KICA8cmRmOkRlc2NyaXB0aW9uIHJkZjphYm91dD0iIgogICAgeG1s\n",
    "bnM6ZXhpZj0iaHR0cDovL25zLmFkb2JlLmNvbS9leGlmLzEuMC8iCiAgICB4bWxuczp0aWZmPSJo\n",
    "dHRwOi8vbnMuYWRvYmUuY29tL3RpZmYvMS4wLyIKICAgIHhtbG5zOnBob3Rvc2hvcD0iaHR0cDov\n",
    "L25zLmFkb2JlLmNvbS9waG90b3Nob3AvMS4wLyIKICAgIHhtbG5zOnhtcD0iaHR0cDovL25zLmFk\n",
    "b2JlLmNvbS94YXAvMS4wLyIKICAgIHhtbG5zOnhtcE1NPSJodHRwOi8vbnMuYWRvYmUuY29tL3hh\n",
    "cC8xLjAvbW0vIgogICAgeG1sbnM6c3RFdnQ9Imh0dHA6Ly9ucy5hZG9iZS5jb20veGFwLzEuMC9z\n",
    "VHlwZS9SZXNvdXJjZUV2ZW50IyIKICAgZXhpZjpQaXhlbFhEaW1lbnNpb249IjExMSIKICAgZXhp\n",
    "ZjpQaXhlbFlEaW1lbnNpb249IjExMCIKICAgZXhpZjpDb2xvclNwYWNlPSIxIgogICB0aWZmOklt\n",
    "YWdlV2lkdGg9IjExMSIKICAgdGlmZjpJbWFnZUxlbmd0aD0iMTEwIgogICB0aWZmOlJlc29sdXRp\n",
    "b25Vbml0PSIyIgogICB0aWZmOlhSZXNvbHV0aW9uPSI3Mi4wIgogICB0aWZmOllSZXNvbHV0aW9u\n",
    "PSI3Mi4wIgogICBwaG90b3Nob3A6Q29sb3JNb2RlPSIzIgogICBwaG90b3Nob3A6SUNDUHJvZmls\n",
    "ZT0ic1JHQiBJRUM2MTk2Ni0yLjEiCiAgIHhtcDpNb2RpZnlEYXRlPSIyMDIwLTAyLTEzVDE2OjQ4\n",
    "OjM2KzAxOjAwIgogICB4bXA6TWV0YWRhdGFEYXRlPSIyMDIwLTAyLTEzVDE2OjQ4OjM2KzAxOjAw\n",
    "Ij4KICAgPHhtcE1NOkhpc3Rvcnk+CiAgICA8cmRmOlNlcT4KICAgICA8cmRmOmxpCiAgICAgIHN0\n",
    "RXZ0OmFjdGlvbj0icHJvZHVjZWQiCiAgICAgIHN0RXZ0OnNvZnR3YXJlQWdlbnQ9IkFmZmluaXR5\n",
    "IERlc2lnbmVyIDEuNy4zIgogICAgICBzdEV2dDp3aGVuPSIyMDIwLTAyLTEzVDE2OjQ4OjM2KzAx\n",
    "OjAwIi8+CiAgICA8L3JkZjpTZXE+CiAgIDwveG1wTU06SGlzdG9yeT4KICA8L3JkZjpEZXNjcmlw\n",
    "dGlvbj4KIDwvcmRmOlJERj4KPC94OnhtcG1ldGE+Cjw/eHBhY2tldCBlbmQ9InIiPz63ZYCCAAAB\n",
    "gWlDQ1BzUkdCIElFQzYxOTY2LTIuMQAAKJF1kc8rRFEUxz9miDCNYmFh8dKwQoMSG4uZ/Cosxii/\n",
    "Nm+eeTNq3szrvZk02SpbRYmNXwv+ArbKWikiJSsLa2KDnvM8NZI5t3PP537vPad7zwVfPKMZdmUY\n",
    "jGzeio1ElJnZOaX6ER8BgoQJqJptTkwNxylrbzdUuPGq061V/ty/VreYtDWoqBEe1EwrLzwqPL6c\n",
    "N13eFG7S0uqi8LFwhyUXFL529YTHTy6nPP5w2YrHouBrEFZSvzjxi7W0ZQjLywkZmYL2cx/3JfXJ\n",
    "7PSUxFbxFmxijBBBYYwhovTRzYDMfXTSQ5esKJMf/s6fJCe5mswmRSyWSJEmT4eoBamelKiLnpSR\n",
    "oej2/29fbb23x6teH4GqB8d5aYPqDfhcd5z3fcf5PAD/PZxlS/m5Peh/FX29pIV2IbgKJ+clLbEF\n",
    "p2vQfGeqlvot+cV9ug7PRxCYhcZLqJ33evazz+EtxFfkqy5gewfa5Xxw4QtXymffaj9hFAAAAAlw\n",
    "SFlzAAALEwAACxMBAJqcGAAAIABJREFUeJzlfXu0b1V13jfX/l2UkAAXiq/EWkg0RqUaBO5BEB+A\n",
    "WKtG6qtWeZnRtCYmgThUvJfRdDQOBAtc2mSEGJuhJn2oNcPwGFZSgmmAG84JAgKS4QPIQ2OVIgIR\n",
    "kbP3mv1jrW+ub+177uUROIfENca5557923vttefzm4+1f4a/R2PpfX+wCfB/6o4XmuHpDuwDYG94\n",
    "3gfue8PSPoDXY74PAAPsO4DfBeAuuH8HwF0wuwue7wLwHbjfCUvXA37j8gfflDfy+R7usI1ewK7G\n",
    "0taLDDYcDM+HAX4oYIcC+WA4ngAA7hPMBgAOdwBe6W78fwLgdTYHcgZMH9fkcwDu9wC+A+5XAX4l\n",
    "HCvL5/2r+x/r5/y7jMcd85a2XbwZ7qcC+LcAngnPgA0AMjxnmBWmuDvgXp7AEpAn8HEsJXge42+4\n",
    "F4bWa8v59XOzem0dZuV4Hh+ApRUAvwXg48vnvkVOenyMxw3zlrZd/NNw/BLS8GZ43hNAIao1BrhP\n",
    "hfAuGgPv/7bUtJDHybjQSqsM9TI/f8OLMpoV5lpo71fhfjaA310+762rjx0VHt7YcOYtbb14APw/\n",
    "AHYG4Ina4NMIS9Q4B+AwS4WBYRKt0p4MyeUnLapmoTGHn9NUUlvNeoZ3x0RjPQPufwXg/cvnv+3D\n",
    "60qkXYwNZd7Stkuejjz9D6ThyJD+PAFpCLPmnmFpUc1gHe5ACoIKgyB/k1FJTKM3xtJ02tBrKjUw\n",
    "tDNXjfV2rk8fhg0/v3z+22RR6z82jHlL2y55CoDrAH8qcg/y3Au4MBt2YlrxZ5XBXnygT+MMjPjO\n",
    "ZnH+GdD8GzXWrAKbVE8VkFNQEYJkZp+F+xuXz3/b3z5qRHmYY0OYt7T1ogFp+CPk6SWFeFMjWIwC\n",
    "Skx8XPkbDazU84KwlprWzU1lZxpz0dz4HLO50LRVGRpgZrX8TmkF03Tk8gUnb4gGzim2PsPSL8Hz\n",
    "S4pvQTFl1gjpOQOwWw32e3D/IOB/6PD7GuKk9Nd/lPHho1zCh6F95t4+h/faFXPItY6i5TzXpzKf\n",
    "GTCtHg7kdz0WJHooY4M07+IvwfCsnTSgEPT7gJ0J+PnXfOCEUIstZ/z+QfD8EcCODlNIwpMRadH+\n",
    "n6uvUpNM9NihTAkbwr+hHR8W5bjOE/cbAM/fA9LBy9tPvPWxp1w/1l3zls689KcAPKuBAxS/FmbQ\n",
    "zrjmA687VxkHAMtnv/42IP0zAF+J2DotGrRPi+a7GL8V0ybocaomcyi/PUtIgP4ar2FdzsA0NoZb\n",
    "qnOGOd8Tnn/lMSTZLsf6m808Pa1QvHIgZ1g1AA6/BZb+064uXT7n9ffB/Vfi+jw1rXA035THerwy\n",
    "K5AoKuMkflQtpp9k0E4/aalqoBemamhS5njto0ylhzTWn3nue9e0SDsEr+gyXXnNWa/1XV8MwOxK\n",
    "5MnDjwFNU+bprzTIMQ0hNPbj6akxK9bnTStpHQInTW2ePP7YltM++pOPiB5/h7H+zFNixqGB0P7r\n",
    "D3b58jlvuBuGv21xW2rapXDeJ0l7iX8LBlHbqKFjM5VqGTT/GdfIOQ3wbH5kBHnkYyM0L6TbyYD2\n",
    "4Qse7PIt7/mfB8GGHwngEUBCgErn9/gZ/5uFSWjnpEHSYdbmVpNr1pAnNd8oPHmvh0WHR2FsgOYl\n",
    "78ICAD6twvMq3P2YLe/91NN2P4Gf2IJqAQ5e0WZ3qve/89SHCpGoBpoPo7kU5jOjA2tpNxfGlucS\n",
    "O74+YwM0L7vnqeUjCenLUvaBpd9Zet+nn7DWpVve+6kluG9rlQY0eM+Umqmv85YWAyT1RWZULVNf\n",
    "F0E7E9XU6LGZyC75zZBk/YsOiwc/5VEelqqgGzyPsKH4O28EeKXDPr/lvZ86HcCO5XPe8N0t7/nk\n",
    "gYCfBPdtSMOmIKrGekDlUZ5pHHOSqiVDQ6oMDyLXSbSp9b6aI63rLxUOF3xkcKx/HXfdmec+eanM\n",
    "5JK7nCTOipPwXAB/CEt5y3s+eS8s7dOZyqzpNEkYIwMZDcQwpZXrZ/Rb02rVzkFCi6mFEWloZjUL\n",
    "uDEAuRy3xDiPHFz/fMeGABavIMMJ77uaGhoC9Jxqa0Pzb6yIa+qr0yoBIG0elOpCnZtZE0B8GEow\n",
    "DrRrVDOT3CeY5pJEX39Srvsdzcwh5ibCBE1XdflGia20YFoub+dS0wL+s56X+nlMtDLCCxEGZZiW\n",
    "hSRn6vF3ERCnP1znse7Mcy+9C5TUIEQgwrFnQjkLkdmofStdgB3aKNobGj37LO4nzCK6JIiJFFgV\n",
    "FmpdvcYkK2NpKBmieXJ7HcYGmM2pl9ZgkPgoVhriGgnC5y0Olsq10aOC5hej4sCAWq5vGeimpXls\n",
    "xVqGFU7EWgcjAiuFYneHw2vMur5jA0pC5s1PqUZQuxRF1thK+1aiilCD7TxJ2CDEZqI6j81carmo\n",
    "TIZoYAKkKmFtroj35uZVwQpm/1+fsQGhgt0F+EoLkGexVaC+DJiEBABCe9pkaFpnpUhqYu48HwBL\n",
    "B3btFWFGJTfJtczBEOF/lK5QUHIaaqhg1exjQ3zeujNv+ezX7wCwZT3uteVd//3nkFc/1CHNAC1j\n",
    "0zr1i+5NeCAJ7yo0BgNbED2P5TfyRijeBlXS12uwvcIgOc+ZSQaaKaU/Y1amlXzanEY5GMWkAj8Y\n",
    "ZnM9R2vZqwckrUWTnTMwbEIgWogZlbjOvSYVKsii6fRJTXU9/Uuv2gTg2e2AmHqa4HLsfqT0NfvJ\n",
    "z3zvkTzeI2be0taL/gmAFwD4EcD38pz3CB8QNbBqTtT5R85Q4qkOtksdTf2Ipq26Vr00+5znJ8Dz\n",
    "i5qpBKKaEAG/oEzXBHWZ130M80gNNjcgLcpzaryow+zpyPnGFn9KImKo2j1NwUS/5fhvYpo+gyH9\n",
    "N6T0OXv2Zx+SA31YzFvadslzkafTYDjO8/QMmDVj0bXIsebFzEaF85GqYkklz9oQagzHdJYy2q0x\n",
    "wazMC0k6M/Ofs/ipSoMk3oHCFOkzHuzDDc+rpVBAgJInwAWgUHNZlVBBIzN530kySWPtPOMYRwB4\n",
    "MobhVOR8Kgxf8FuO/1l7zmWffzB+PCTmLW296KcA/Cry+EZ3T5Y2ITqVE3srrVTEA9ExgwE0xqnk\n",
    "A/DKAEWTXe1NiqkkUszhHbEjRLCESDzrCHOVmxWIuayhyDzV7rRZ0M/1dTW/mWWY34u1Rs0WJUky\n",
    "hPmsP0Xgng9g2W865gKkdKY993/vcrPLgwKWpW2XnAazm+H+ZqRFsnkPiDNRO7RUF9vRI+huaC0W\n",
    "O60CzhhsjQxFxFGM8yDxWDU9adGIaJU48WQsrgoTtOJe40P2gFptKrKUyrF54G1SPO7mUZ/Ke6fW\n",
    "tMTPh6Ektdlvw2ec6ArQaJB9APAumH3Cbz52l3XC3TJvaetF5wDYDs8p6mXOHTq5Pmiu8HmqWZOp\n",
    "ma2utQBlkWmoi2fCl5IM0VhJWelxDs1fMl6DVWZWjexKPgI+wtQVs12sKBkx1tuKH2Uuk+uf+1lp\n",
    "pIoRjVAtPsS4Kv5fsMBClGGamvkv87wW7h/aFX/WNJtLWy9eAPnDMDslNElQGptenf6LD2KMjypx\n",
    "tR1PC6gBSKb2kNTSaSzoL1Jn1KxZlgVDIcA0NkKQwMEsxmqV4SRK5EfJoLKZxYZF067a09btTCKo\n",
    "0awQ5LY6VGiGoWlgqpYkz0KRaSqfASJ0BsB/1r/wsm/a8z+3bc6ntTXP0llAOqXFRnG8oLA8VdNS\n",
    "Vh3mJczpPL1Ua2eay9SgmAum1NHkkNH0ozQt0XPJuI0mVh6c9yUz6V+8AhptgIo8p7d1eC4+XEtO\n",
    "XaUjt7/Dt5Oq2maB8jyhmXIvjhAKNFNKC1TQ6lb/wste+qDMW9p26cHw6fQwWyGZU0VcxVx2N2cQ\n",
    "PG/0mZswhda6BYvt7juVdupc02qbs0tAo80BZaBIddTrhGA2NEJV0112IjUA07RNLEeUoxTQ9DFe\n",
    "WS/jxWo1hmrgOsBEoAVhNh8jiaYygZB+bbfMW9p2scHzhQAWXFTEbZVAJZMucVkEwd7MU0dINGaE\n",
    "mVMgUn0XyyoEHurjorIdpgTR/cw+mLgZeq1QjYxTKqGs7PcrlkRQL58nykv1+V3vMbVzZ7ucwqpM\n",
    "UzmdoYIKb3QQzEDPsEYpqtDiKL/5uFfuknlwfzs8H9mYUbuZxdwZWwd0oXyIIK7U4whcaMLU9HRz\n",
    "eDOjMZcjup6VgXwgvQcRZAT6aASJ0hAJS186iSZhdo6uR4QjGKv+G3IO+nZ4Pi5Nfao0YDVfa4cE\n",
    "LDxOS8C5p/H9azJvaetF+8HzOVGnmka0doXysNwPHg+VhjUemGiTU1szF3NU2ZlVl9jMW0tCwOc6\n",
    "r2p7hAre5gsi8wmHdj4tQzAacg1ijYY1BDJMKoShJmHL0Cbi/MmaeeZzMDTIUwUt1LihziXuIwlj\n",
    "i+97oX/hZSfsxDwAr0da7A+vm/bVRDWSCFDxtkitXishwyyuUadTgpJwFIaYYwZCyCyb/9Asz7Sb\n",
    "Bd9KYBZPkYYGGQQYmACyZjabiQ2hCCwgXdl5ujMIRUtBk06XQS0jIEvCKPrJCOyBTvsbuj1pDebZ\n",
    "axRVlcfPwazYE6caFLtW6cBpwggWZoleMoeL73wTmu0PM6rVcGEGGRuwXvxuWIYJGDbB81iSx1Uo\n",
    "zQzclWRm8Vw2LJRATUs7nyrWJExvmNK2xcvQC2wgXQfGSTatCA1oNlF9ZeVD0GQKIXmpX390itVs\n",
    "ed+n9wT8WBLLp7FpWP3tObfEcOcjKqHClXlPhDhXfGcSUxupJjEt4QuGZi41YzNvoaD5oykXrbe0\n",
    "6PKv7r2AEJC1JAN9GgVKgImJC6BFKWu6a+U3f+HuYJ7jhxroIAPQJ+qDJmjAJqWegdPUBIfmMw37\n",
    "AnaIitIxAPYMems3sRJZQYQGw47mo8BFUmMk2CYzuz0ALueiI3znc9RfMfaKdjxv6xO/2e2FMPRz\n",
    "1vCnnKoMErChg3Nr7MvKhPtt/cnYjNnlSAnYJD6a9NAYOdamzEqx3qAP8HKgMe81ZS0JDm/pIfqP\n",
    "eWNsgAPmLtE+VyDCDAcJQyIGQqsPEYlqMUXA7CFl9yofQs1y+EoLKbeaj6Rv8067lbo0+anN26Fe\n",
    "yO96r7SogGIA4F/pGOX5yQ0zoAnjRDch1omt+cMafaH8m0E7Ndi9MG9p60UGz6/2PNYgvEqsEpSp\n",
    "LDJTt0JRGud+kAuIWlldTCR1yUlv53BOoAGhPJZjBACRlSdjUyM2rwPi/OLnNOCmkJBpMxVRJmpI\n",
    "MNV10OfHtjAHHB/tJ7FDIpaLa9GupwmdRrFimIVD4mLCDdWflI7y64/elAAcbGnT07pMAaVOW73Z\n",
    "EtehTTWHEJ8DMWEm51eNC5RIAqqJFOK6EI+f5zESyEHI3cRlDMDL+1x0syUagTSeUs1IA0pTkyLa\n",
    "qlFtrdevXPgLl80E4NAODSexPjS7QwVvAzu7s1gSQdFAu74h2L3gfngC8AyP+MmDwxEuKEgwiZnI\n",
    "oFhY7n9Dru2y8MLIebqL85LIEdPVefOIYqqHnshdeGJ9bFeRqWkIQI2nCeR6w0QrwJgxrSsLATA7\n",
    "u+PbLcfvA7MXt8Ba/CkZMQzSWs+YlWkwEX5eT6aPYwM9ln5iAfcnGayP39Ls5TV8OB8BWwBWzUaH\n",
    "nOjkO6kUTSDQQBAUruhtQlTd1ZSF7xDtjHQa47oGcFwyQy6C4socCkggOfU3uS8ok9nRgsF1AnC/\n",
    "Co5PdWTK+U0wPHEnwEJGKWPU6hDkaSot3mihvjeu2y+55wN6U5b6XJ0iHfo++sQOneXGJI3/mAmx\n",
    "AQ3V5fZDn8bqt27yiLixmkma6nnch5pvZes56muvQivFZ3DNaiki1qoE5prYOa2Zm2YlbgDw6pUL\n",
    "3ynEAmB2SnM3aDSdB9vamhHgSxirrznRYyzowvZPgD/JmeDtHnbmz8qCZwxV/8HfDGylKKsxEc3g\n",
    "fN4u3yemMF4nlXsiaPAPlEqH50qvYq7DelDbg2FTM8XhRyqj0tBnepJaAoIGfBnA8SsXvrPFdgD8\n",
    "pmNOgOFFYDhkQhemuvi8I4W2Mneeu233apoIIbn7fgnuB4APTM1R6Qy4LhNxMRGDCfoLPyPMzFPf\n",
    "rxEmgMldQaJdloMIsoYc1VRHpcPLG5FomlvFm+ZbQBj3IYRGpGYeO8FySV/NUnSwDPffgOfDVi58\n",
    "57c6xt1y/N5I6deDaewYMDT/FiBQ6McffemPXk9hXXDtwdj9F7DhgCKxRIaE/VRXJaS3vF0XB1VH\n",
    "qy8oXdPUKKiRRQdA8fbAZs2UpiTINcGqz/W4m9ellDY9r1UNs6H4QEW1FJBAyxA0twnRbq9pwPJW\n",
    "ps/B/d+tXPjOP8NaI+f/iGQ/iuw1IS0WbBiKpnVIl5ZlRKcsVgVrHAvDXOitRemU9l8grz6ptB2o\n",
    "TxATGnvGrRJWu4plIT5jfqOsCINoH48HoYTp80KofqadyqGB3PJn3Y8Hg6g9VZvYHjgfLn7H7Btw\n",
    "fAbApUC6fOXCd6z5dj//4nEJjv8M+M9h8mak4vmtCUxknKgIEkrlXJjOdghqIbCz0PsEjHm/BVLR\n",
    "vI6ovtODtGNm4NbeLh5TbmlGJgCQmGMtnoapSwCQYelr8PwXgN0Oz7fD7K8Af6CthUAJMNkn7gGk\n",
    "ir+1oXaGdY7HKbVNSLUoCn8ANvw/AN8A8KWV3/z5uXeeMe4VewD4r0h4Y6EJetPfIdrUEOQglksz\n",
    "TzTnzG+yd5XghY26AJBs/wXcD2iV4txu0nG6SvicYRFbAXBrksZrWVCcB9uxWJqA4Vb4tB2eP7p8\n",
    "wanf3R3BHg/Dv/iKAYZ/AeAMTNMhzS2gPN+waIzScABoTCATeYznpxqGgGCtCoFBqhEZsLRfeYWs\n",
    "2RNCAl1UOfyeMqT+bUCX0KW50tTaTv8XiSztENfBcBZ8+vTy9pN7yP04HP7F4/aCpVORp9PhdlAD\n",
    "EIqIKw0m8Z0BSmoBNlnNsFifOovwoTJ8IUKgKLT8f7GApW8jT3t1BVTaVUV+mrkgpI662moDFVrR\n",
    "ZjosNLf6xDwBw/AJ5Omk5e0nP7Bbgt36WoPj2UjpqQD+Edw3qa8Ln7xYAKurAqYaIu2C/m5y75+R\n",
    "xwAgT3vA7BmwdCA8HwjgQLg/Dcy0s0o+DDWwJt1nYEVjNEuAMVTKxVrRtC4WBaRELnMWMpDp4bL8\n",
    "jgXc7wTs6bH4OVzv8p2KzrwRJikqym3xSowIrg2A/wbcf3lX2ua3n/DDcH8F3P85gFdhSE9ppRNF\n",
    "sYKIV1clTuR5NF1o66af0fgt1wLpODYm0DoshkJI+u089oRG1SICuoiXp1Y1j1dEyr1ppZgO0zQa\n",
    "aa3YgoicG04Ndyxg9u0mhYKGIoshMVDEdSSIossZKtQ4K2fV2g8tX3DKL67JtNtedxhS+jXk/DJ4\n",
    "3iPMM32Hmhb2oOi7U0ZaAAVSQORgdcOH1hR5zWJRN4IAARY4ZwgKWpYj6AW00EJACIWAvmwcWwMu\n",
    "QwKuTWNb3oda7UJfPlv2b1XNkws7rRHg0pkXQXHsPiYRyVQyXetw7l+DpXfvxLTbT9gHwFkoX4RR\n",
    "uBPhgtwvbsv1oPkM9oOQoMq8ObKLG3tjqhKGWhfPVCV+FLRHLaegJ12P9UIUAizaxmegxiUqhstn\n",
    "uc3lst7yHHcsANxZ8o6y6aNDSJVBXfZCCShmkk45pcKDQJxBzF9cvuDke2eMezLMrgTwzK5NgPPx\n",
    "4RYJmBxRP9xUY9Ms4QnkWhKN/khNJC1EEFilXyxLlzRGAxqrouFRRKXgcK6ZBSsLateR2fVwFFwZ\n",
    "zpBRhvLcerwo0h0LAN8ugEO+SEIz+J2ZRNOmSOmkoEVZHOcRphe/tGP5gpP/oGPcrT+zL+CXwf2Z\n",
    "gUJpWiiB9K9Tbg8ENLMXcafJGivT1io30UcpWOEPG4KzfGbWgMJ8h5TGcXm2PiL30LrKtGkqjbWj\n",
    "Jjkga8lFq8kwVt/p64Pu9q0En+4MBjHumL8ag1II1NZzmjCaqGpC9c14ob0RoH+kY9xtrxswDJdi\n",
    "ys+PhyUImGfY1VxqtkQDYK41KumpgQEOJocJyghCumNimgIhVwZyz0HkQ2XOiJUpUFPzjWRaZLFQ\n",
    "zaSXOQZZv5pOBTkqbFXzElABC9M1hgL9g8piOkmAcN6cXJtnNBasmgi7HzZ8sqck3oBpPLKLIwfZ\n",
    "vcNANsydEHyetVDfxnP1Nz9T4k/in6PBNzXfM69+Z0oQL7PGFGZEGLsF4+u5C/nqAQbwleQlkSLm\n",
    "VTW1s25MkjgTH99K8PzVDmzk3BjVDXX+ufkeGOLteZGzEwRaNPZPlrefeM9swjM6BpBZ08xEa72N\n",
    "95//n0HxnFkkRgfRhSGGHq6vFRMGgEDTyHk7BURoKEya0spiwsO6uIAs5aS3+QKJkt402QYMw18m\n",
    "pGEZebyvI1j7o03UESv3Gth1JgtYCH/p13b0uO11rwT8BYHq4vz6QLx1wO46V0BsIZzC/W4blyDk\n",
    "WKcgOYIE9enh66oQae1xlFdakVFKM01vxT1TLeUQnHhDnJoLzt7ckJpJ9b2ct1igr9uhV9+Ulj/4\n",
    "plUAV3WZFd0xoxl+QJx6bsejoCmV8ngwA/J0PfpxcixwsakndHBOhaTeN8ykCJKO+JvmGmJWvfdN\n",
    "ASzQGNglJyp9Ax2KT5s34ur9wxx7M6lMKJNJfL7AAxJGwFr2SunY3+9SgDFVnq5oDEqifSqp9WZE\n",
    "f3GMNlhU2oSRAJCG/9stIqVntUR/tfe6NzsyN7UgSUiuplkbc3ZiHnpBMjTwQIZGwL5mSNMj2MUi\n",
    "yBECADQBoECr1lHAIheZm5nk/8NXo6BP0s1zq3bMM15lfZc05qXhisZVgeBRVYDEb/Q3JFqIaCOU\n",
    "nlMWc1fHvDwd1CVdqRV8IKKxCGAZpOaWodCM/NyUdRqUZMuwmOEQPPVR4kuj3ua6T6DXPIKfeHPS\n",
    "fI66lkGEkMdHdo95KGKPgkla702/+31I6Y8a89yvQ/ny22abI7OAtkDd/xYpKzKbN1KmGh8s3vDj\n",
    "t/3MZjj2bcGzEE/TV3QJaq7Y+kaidGhUND0YMglqI9Pq85lJ3CYok9pIN9Jp1Ow3nzPiSgF19OFz\n",
    "v6sKMBc0bUqKBAM/j2sut8N23B/MWz73LRMs/Z82GdC+uUoXK46XQEXjriTZhsiG74TgDmrghLGY\n",
    "iTlSLbKSWxzHpnV8yEmkn0SMPKAAhjnhVCMDLddniFwi56wCM6kJFqao1g5KL2o5BG3Wv8dVRCIc\n",
    "JJUCk1mTMUfskygmM5hXz/5YOFCF13yIMIlomhgtDLwBfYpu/XJ0oUfO+4VvzD5rBec9vQSuSRav\n",
    "64k+FN6mmqbIO3b3Q2icSnI8q65bwoa2A6jXHn6mQhGJ+TrXKAham5LpUuiS3AXMiMCwMVefs8x1\n",
    "Jwy/z0cL5i2f+5ZPw+y64oemxkQ6UWk/6Fr4qDnKxMhs5J0/s2QN+ub2m3NrVVmddWR/rIUF/FsZ\n",
    "qyCmi9vE5ykQYtigoQQ1idLO8yaGRPO38KKh4PkboCZBx/W7bzvUazNTyaGC1hIT77Ety4EfNKgD\n",
    "3M9sNTn1bWhMUCIFKqLHFScf/hPw7utf0AhkqaG6CHB5L5F0EltbCCaZk+uZ5N4hsZAsiAlheJ0w\n",
    "vGNcbhkTNWls7VBfpfGfZlE81xcEKHOsjxk9NxrPn8tFcFK6CrMUY8e85fPe+r/g+erS4azZjdSY\n",
    "GYvNTYIiEW3N7xEYwNF9U4sGoWqidX+CnsuHzlMzR3ywrmXQRYgg73KhZgnxBvF9LGdpBkY1Xv3R\n",
    "HM1Sc8ISiL/TZ1P0Teuim0cIqjQQLzfisRHu77At13Tgode8Ms7s0lGdJnJRs4ckoQn7eWPV0m5Q\n",
    "CFzMjsRt6j+AhiTVVDIpQPMSx/U2FAaa/9yuiWwImhVQwKM51KCWmN7QGAI69BmmoAFmbkPWQlMM\n",
    "VFAmrqrDHNhuW665eUbEnZm3fN5b/xjwDwBoTOs2XpAp9ccnRCcyH2JuuvoAU/yo9egqJJAbSGbS\n",
    "3z1Ybr6QhKT0x1rElFVaYlhIcD2188NyaIoqt/NCKyhg3uI30on5x8izyrwmAqvxnPp0BuYqjO4f\n",
    "AXzrnE9rMg8Als8/cSty/p0OXWljT7xPE+hfj8jPw8ije+kOmaRARcERTIgukqdxnprnTtO8wvCM\n",
    "9ooMa2YptGvs51V/C/Smscs00fzJ/+daT589jwMpH9pOou4jwikTkDQBns+2Lde83ZZW5lu2ds28\n",
    "uuB/A/dLupvoj/rEWCQX1kIOs/JqqI55Qu8ABmqeDU0zgXaPJJoffnBViGZFswiK6HcDHBFooEl4\n",
    "aCzavES2fI400+5AvqjCQBJInMqmJK4hsk2VftpAO8z83jRmpOE0W1p53675sxvmLZ9/4gTPbwL8\n",
    "XHBbTZiuhO6Vv2ivtepeb5jl1VA6KImguRCkN1Vgopn6PBccawxhvEn/BbTYLszXDHIHUhbkGmGD\n",
    "mPqowtc5dN84n4OVDsfOn2lBNWpyMyTNzwvKzAA+jsWm59mWa3b53bkcu33T7fIFp9wP4N1bTv/Y\n",
    "J2DpvyCPz+/MpCR1Tb4lK76MHrLIGBbXtIQ0mlkmTJ/7A/VD04yJg5hQdnsRQDCvyevDp6HPuJDA\n",
    "4wNVe8UsEq3SpLKqDjRzP9hsF2zVWAIwmzFXqxXT9BcwuxzAdjtsxy0PxrSHxDyO5e0nX7vl9N89\n",
    "BEgvhftbAX8VgKdQIt3Lq33L/lMBDRHIayAKRCGSqkImAD3TFDoHEwW0kDGT9L2oBho+gWHYEeBh\n",
    "sWi9MBQSZUDsfvW2VgIJIuMQskWLcRXR8hmAZqYZzxZmOdwfwDR9F8C9SOkGW1r+y4fKMB0P+QXh\n",
    "y9tPygCuqD/YctrHnuh5/DEz7GlW3nhbHrq+PTaXLVaFFvmvu8l0E2UMYRglUomrCDE6r6qTV+bT\n",
    "XBWze4U97/Lffvhk+fsxHvGr+ZcvOPl+AF992Be6l6B+WDTzq8XOMCVjCyOUOdos1OUE0cy4zv0P\n",
    "eOwGbT5GQ512Za/2AAAGk0lEQVR2OO4ZGKH/UShOfxdZixpzAQ3QRIqL6aq5v/2HNdb96fwrr34F\n",
    "3C/baRNI53/q79i5RGbl5gcJRgABQESh4HW3I093xHxd2kv8FNegIQGArg2ReV7W/SLQnmjuT7cX\n",
    "XrXjsaVeP9b/62imsTl7LbQqDGeQ2qW9rOQkycDYl+C9CdXCMXAggAM7eE4NjS1WRM4AWDW3es9x\n",
    "td2b3+MwocVzPSredz3JCGyE2eQGQaBVxiMGm2bmEQ2SR6uhN3PK7mOgbhARwKNF4C6LgYY0x1W5\n",
    "t68RlqDX1s7k13lih5CvuxVbf+Ypk9SfsUJBItGE6WZ65hKZ+1ToD8yakrw3icyBRnNT6lNs9JXU\n",
    "NveyvUtrl5C5w+dKqLDOYwMAC9NT1jSA2580Oa1EjJTS2ALmlErHlTID9Th7JSN88AZgmDXRF5Jq\n",
    "Wajrz0G/b0IFjvFdtGesv+atv8/TLEPEc9REOYd5zAgnagDNrjNDM5nRGzk086iM7zrMpMkpdqJK\n",
    "vbKbU2NRQ7z4lOerSd0A1dsYs6lMYgaCCFELrgos6F/4uZpLVhk08cuhQMhQvy2lfqbfIMJMUNsj\n",
    "cTeGRY4LDW0bFhm32NTuP6/9rcNYf+YBaBCdYEUaf0ikaI+QGhlfREPma8CulXXdr85qNFAB0GpD\n",
    "lgGOohK+imH4VeR8oB161b4YV/fGlI9DnkohNFJh6NdfKgQ/AGZTW/uGRZ/MjRqcNW2wVEQs59q+\n",
    "gIoyB9m0QfOmYCVJrKjZGGvmcKgvIcAEGFZhdrQdcuU1XKkdevV3AVzuf/aiQ5HzRUjp+NB83XK8\n",
    "QWNjMiwVrLXyDNC1FnK/OZFp9t7XAdJdVf+eo1c1qTw/kGdq8yxq/S8N71fGdUs+bMf3AbwdZndj\n",
    "sUkajAQo/UCECu5TfZVHe/goflauRlGTCBGI/dkKSHgNzaXWzroYTyr9+jIAfU2w++/tbtl22I6/\n",
    "wTRd0Xp5INmetCFmcwN8nt/XvZ4wsiJSzQYEkqOdQ7MaxUuaSumvpK/TnpfwaeIvQ+MBwO+1Q/7k\n",
    "9gdfut8Qr9HXtdInr/PYAObZtyNmA1qKS7c/aexGLQlGQNJqEo8FcrWdi6xw6ZWEZEyMgOWH/fqX\n",
    "7P3gS7cf7RLpIWAJbSHrNzbCbH4Znr8evfpEfFrCGdQMQpjkLTaMzIYyCYIy69BOLrO+Z5MhhrvB\n",
    "84t3u+xrj0wAjgqB0op/ue/du7v+sRjrzjx7zmUO2MVdjyT7Q7gnnuYsDe0ryRTSR2c0FKrLXRgC\n",
    "VOSq/nGU6reWobJv9+uO/qFdLtz9l5Hzc7rNpYCGLd94lEn1oGNj4jzDeRiG73WxlrYR6LtTmDCm\n",
    "BmkQP++P0ewJMygEL9oyr2kwCozZMwH/rK8ccZBO6StHJP/8Ue9GSmfvFEMyP+r5S3b4n/75Y0ix\n",
    "Ncf6Z1Pr8D9/5RkYxw/0mxGtT5kB0jAk8eFik/SPiG8EehPLlxPoFuac+1ogEwEAanvgfUjpapjd\n",
    "gGl6KoAjYPbj8ebZiCnRwg/D6ba0csFjRqxdjI1j3s3HLmB2NWCHRygQucO6rD53KIyaSOiIJgKF\n",
    "As3EMl/qKHOPYvKY01TgpALSNRMxeSCC1HKnfwzgWFtaXne4uUHpMcCed/mI7McC/tnQgvCD9f8B\n",
    "asTfjWML4COrwmBZ0l1R2/NWr1NNBXrww7+jIq/t+DO/GUjX/wbu/3IjGAdsIPMAwA6+/F64vwZm\n",
    "H44QAOjLQpF+8p7w1AYWUMlUoCWcu9hLEWZifNfQbAAXVvA5h7THq8bBrgKwZEesfPPRpcpDHxtm\n",
    "NufDb3z5v4bZmQD+cSMS0IUCWkvr+lG8wfcI0tFMJzvRtD+FrRgRuM+Tzt7/EOWaZZidBc//3pZW\n",
    "NrRF7XHDPADwm47ZBPeTYHYGzH5izdoc2x9iP1xq7RDUIiagqc3Zm0YpwGGCW30cINoOFZLvIaWP\n",
    "w/Ov25bl+XtlNmQ8rpjH4Te+fADwZri/A8DhAPbYSdvU5AHS1ETtmyQmNOkFTTMhGNCFGLw2Kh7+\n",
    "ZQC/hewftSNW7lp7xRszHpfM0+E3vOSJsHQ4zI6C+4uR8xEw7BMJaIYE829GjpeX1oCeGsgsjVbY\n",
    "yeSUvg/3GwFcC7NrMY7XArjJjljx3Sxxw8bjnnnz4dcdnWA4GJZ+GsD+yHlfmG0GsBmeNwPYF47N\n",
    "SGkzxtXN1UfdDUv3IOe7Ybgb7vfUdNY9MLsbwF8j588DuMkO/9PV3d3/8TT+P2x6AX5Riia7AAAA\n",
    "AElFTkSuQmCC\" style=\"display:block;margin:auto;width:10%\"/>\n",
    "                        <br>\n",
    "                        <div style=\"text-align:center; font-size:200%;\">\n",
    "                        <b>test</b>\n",
    "                        </div>\n",
    "                        <br/>\n",
    "                        <div style=\"text-align:center;\">Your name</div>\n",
    "                        <br/>\n",
    "                        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "596d16f3",
   "metadata": {},
   "source": [
    "## Introduction to Reinforcement Learning\n",
    "\n",
    "- Reinforcement Learning (RL) is a branch of machine learning where agents learn to make decisions\n",
    "\n",
    "- It involves agents interacting with an environment to achieve a goal\n",
    "\n",
    "- Deep Q-Networks (DQN) are a popular and effective technique in RL\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d223a461",
   "metadata": {},
   "source": [
    "## Setting up the Environment\n",
    "\n",
    "- We'll be using the OpenAI Gym environment for our DQN example\n",
    "\n",
    "- Gym provides various environments to test and train RL algorithms\n",
    "\n",
    "- It's a critical tool for prototyping and validating AI systems\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36270e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import collections\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a5fbd8a",
   "metadata": {},
   "source": [
    "## Implementing the DQN Agent\n",
    "\n",
    "- First, import necessary libraries for neural network implementation and optimization\n",
    "\n",
    "- `torch` and its modules will be used for creating and training the neural network\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13557bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b48110c8",
   "metadata": {},
   "source": [
    "## Neural Network for Q-Value Prediction\n",
    "\n",
    "- The brain of our DQN Agent is a neural network that predicts Q-Values\n",
    "\n",
    "- Q-Values represent the \"quality\" or value of taking an action in a given state\n",
    "\n",
    "- Our network will be a simple feedforward neural network\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ba00dd6",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "- In this section, we laid the groundwork for creating a DQN Agent\n",
    "\n",
    "- We introduced the Gym environment and necessary PyTorch tools\n",
    "\n",
    "- The next steps include designing the neural network architecture and training the DQN Agent\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44ffcd9e",
   "metadata": {},
   "source": [
    "## Hyperparameters in DQN\n",
    "\n",
    "- Hyperparameters are crucial in Reinforcement Learning as they directly influence the performance of the model.\n",
    "\n",
    "- In this section, we will discuss why we choose specific values for our DQN model's hyperparameters.\n",
    "\n",
    "- Understanding these will aid in fine-tuning for different tasks and environments.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7de2d84b",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.0005\n",
    "gamma         = 0.98\n",
    "buffer_limit  = 50000\n",
    "batch_size    = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d567a60",
   "metadata": {},
   "source": [
    "## Understanding Learning Rate\n",
    "\n",
    "- The learning rate determines how much we adjust the weights of our network with respect the loss gradient.\n",
    "\n",
    "- A smaller value like 0.0005 ensures that we make smaller updates, preventing overshooting minimum.\n",
    "\n",
    "- This value might need adjustments based on the specific characteristics of the environment.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e8139c0",
   "metadata": {},
   "source": [
    "## The Role of Gamma (Discount Factor)\n",
    "\n",
    "- Gamma (γ) determines the importance we give to future rewards. A high value approximates to considering long-term gains.\n",
    "\n",
    "- Here, we use 0.98 to indicate that future rewards are very important but slightly less than immediate rewards.\n",
    "\n",
    "- Adjusting gamma involves balancing between short-term and long-term benefits.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d8dfc23",
   "metadata": {},
   "source": [
    "## Buffer Limit and Experience Replay\n",
    "\n",
    "- The buffer limit sets the size of the replay buffer, where we store experiences to sample from later.\n",
    "\n",
    "- A large buffer, like 50000, allows the agent to learn from a wide range of past experiences, improving generalization.\n",
    "\n",
    "- This parameter needs to be large enough to contain a diverse set of experiences but balanced to avoid immense memory usage.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c60b06c0",
   "metadata": {},
   "source": [
    "## Batch Size for Learning\n",
    "\n",
    "- Batch size determines how many experiences we sample from the buffer to update the network at a time.\n",
    "\n",
    "- Using a moderate size like 32 helps in stabilizing the learning process.\n",
    "\n",
    "- It balances computational efficiency with the benefit of learning from multiple experiences simultaneously.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5671de24",
   "metadata": {},
   "source": [
    "## Introduction to Replay Buffers in DQN\n",
    "\n",
    "- Replay Buffers are an essential component in Deep Q-Networks (DQN), enabling efficient data reuse and stability in learning.\n",
    "\n",
    "- They store experience tuples encountered by the agent, allowing for the random sampling of minibatches, which breaks correlations in the observation sequence.\n",
    "\n",
    "- In this section, we'll implement a simple yet functional Replay Buffer using Python's collections and PyTorch.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28d23b30",
   "metadata": {},
   "source": [
    "## Defining the Replay Buffer Class\n",
    "\n",
    "- The Replay Buffer class is initiated to store transitions with a specified maximum length.\n",
    "\n",
    "- It supports adding individual transitions to the buffer and sampling a random minibatch of transitions for training.\n",
    "\n",
    "- We'll explore the class structure and its critical methods: initialization, put, sample, and size.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4e571c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import random\n",
    "import torch\n",
    "\n",
    "buffer_limit = 50000  # Define the maximum size of the buffer\n",
    "\n",
    "class ReplayBuffer():\n",
    "    # Initialize the buffer with a fixed maximum length\n",
    "    def __init__(self):\n",
    "        self.buffer = collections.deque(maxlen=buffer_limit)\n",
    "    \n",
    "    # Method to add a transition to the buffer\n",
    "    def put(self, transition):\n",
    "        self.buffer.append(transition)\n",
    "    \n",
    "    # Method to sample a minibatch of transitions\n",
    "    def sample(self, n):\n",
    "        mini_batch = random.sample(self.buffer, n)\n",
    "        s_lst, a_lst, r_lst, s_prime_lst, done_mask_lst = [], [], [], [], []\n",
    "        \n",
    "        for transition in mini_batch:\n",
    "            s, a, r, s_prime, done_mask = transition\n",
    "            s_lst.append(s)\n",
    "            a_lst.append([a])\n",
    "            r_lst.append([r])\n",
    "            s_prime_lst.append(s_prime)\n",
    "            done_mask_lst.append([done_mask])\n",
    "\n",
    "        # Transform lists into PyTorch tensors for the neural network training\n",
    "        return torch.tensor(s_lst, dtype=torch.float), torch.tensor(a_lst), \\\n",
    "               torch.tensor(r_lst), torch.tensor(s_prime_lst, dtype=torch.float), \\\n",
    "               torch.tensor(done_mask_lst)\n",
    "    \n",
    "    # Method to get the current size of the buffer\n",
    "    def size(self):\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96d7ad1c",
   "metadata": {},
   "source": [
    "## Functionality of the Replay Buffer\n",
    "\n",
    "- The `put` method allows for the transition data to be stored in the buffer until it reaches its limit.\n",
    "\n",
    "- The `sample` method randomly retrieves a subset of the buffer, which is used to train the DQN. It returns the components of the transitions as separate PyTorch tensors.\n",
    "\n",
    "- With these tensors, the DQN can learn from a diverse set of experiences, improving stability and efficiency in learning.\n",
    "\n",
    "- The `size` method returns the current number of transitions stored, useful for monitoring the buffer's status.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6726c6f9",
   "metadata": {},
   "source": [
    "## Introduction to DQN\n",
    "\n",
    "- DQN stands for Deep Q-Network\n",
    "\n",
    "- It's an algorithm that combines Q-Learning with deep neural networks\n",
    "\n",
    "- DQNs can handle high-dimensional, continuous spaces\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cdbaa01",
   "metadata": {},
   "source": [
    "## Defining a Q-Network\n",
    "\n",
    "- We define a neural network model to approximate the Q-function\n",
    "\n",
    "- The neural network takes the state as input and outputs the Q-value for each action\n",
    "\n",
    "- Let's start by defining our Q-Network using PyTorch\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d56e3935",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "\n",
    "class Qnet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Qnet, self).__init__()\n",
    "        self.fc1 = nn.Linear(4, 128)\n",
    "        self.fc2 = nn.Linear(128, 128)\n",
    "        self.fc3 = nn.Linear(128, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56d03d38",
   "metadata": {},
   "source": [
    "## Network Forward Pass\n",
    "\n",
    "- The forward method propagates the input through the network\n",
    "\n",
    "- Activation functions like ReLU are used for non-linearity\n",
    "\n",
    "- The output layer does not use an activation function as we are predicting Q-values\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f18cfafd",
   "metadata": {},
   "outputs": [],
   "source": [
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b0dc5e8",
   "metadata": {},
   "source": [
    "## Action Selection\n",
    "\n",
    "- DQNs select actions based on the epsilon-greedy policy\n",
    "\n",
    "- With a probability of epsilon, we select a random action\n",
    "\n",
    "- Otherwise, we choose the action with the highest Q-value\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ec8b8a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "    def sample_action(self, obs, epsilon):\n",
    "        out = self.forward(obs)\n",
    "        coin = random.random()\n",
    "        if coin < epsilon:\n",
    "            return random.randint(0,1)\n",
    "        else: \n",
    "            return out.argmax().item()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a919bad",
   "metadata": {},
   "source": [
    "## Recap and Next Steps\n",
    "\n",
    "- We've defined our Qnet class with methods for forward pass and action selection\n",
    "\n",
    "- Qnet will serve as the backbone for our DQN agent\n",
    "\n",
    "- Next, we will explore how to train this network and implement the DQN algorithm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96d1c94a",
   "metadata": {},
   "source": [
    "## Training a DQN - Overview\n",
    "\n",
    "- Discuss the purpose and components of the `train()` function\n",
    "\n",
    "- Explain the critical steps in the Deep Q-Network (DQN) training loop\n",
    "\n",
    "- Introduce the parameters and tools used in training a DQN\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e12fb050",
   "metadata": {},
   "source": [
    "## Memory Sampling\n",
    "\n",
    "- Memory sampling is crucial for breaking correlation between consecutive samples\n",
    "\n",
    "- A random batch of experiences is drawn from the memory buffer\n",
    "\n",
    "- This technique is also known as Experience Replay\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ed440e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(q, q_target, memory, optimizer):\n",
    "    for i in range(10):\n",
    "        s,a,r,s_prime,done_mask = memory.sample(batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1078a017",
   "metadata": {},
   "source": [
    "## Calculating Q-values\n",
    "\n",
    "- Calculating current and future Q-values using the DQN and target DQN\n",
    "\n",
    "- Utilizing `gather` to select the Q-value for the action taken\n",
    "\n",
    "- Computing the maximum Q-value for the next state\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42778e4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "        q_out = q(s)\n",
    "        q_a = q_out.gather(1,a)\n",
    "        max_q_prime = q_target(s_prime).max(1)[0].unsqueeze(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab22891a",
   "metadata": {},
   "source": [
    "## Computing the Loss\n",
    "\n",
    "- Formulating the loss as the difference between the Q-values and the target values\n",
    "\n",
    "- Target values are computed considering the reward and discounted future Q-values\n",
    "\n",
    "- Usage of Smooth L1 Loss for stable training\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2bfc641",
   "metadata": {},
   "outputs": [],
   "source": [
    "        target = r + gamma * max_q_prime * done_mask\n",
    "        loss = F.smooth_l1_loss(q_a, target)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fd27d1a",
   "metadata": {},
   "source": [
    "## Optimizing the Network\n",
    "\n",
    "- Zeroing gradients to prevent accumulation\n",
    "\n",
    "- Performing backpropagation to compute gradients\n",
    "\n",
    "- Updating network weights with a step of the optimizer\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65a99d16",
   "metadata": {},
   "outputs": [],
   "source": [
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "046dac1b",
   "metadata": {},
   "source": [
    "## Introduction to DQN (Deep Q-Network)\n",
    "\n",
    "- The DQN algorithm combines Q-learning with deep neural networks to let the agent learn optimal policies.\n",
    "\n",
    "- Here, we implement a DQN to play CartPole, a balancing pole game.\n",
    "\n",
    "- We'll start with setting up the environment and initializing our network.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7f92023",
   "metadata": {},
   "source": [
    "## Setting Up the CartPole Environment\n",
    "\n",
    "- We use Gym, a toolkit for developing and comparing reinforcement learning algorithms.\n",
    "\n",
    "- The initial setup involves creating the environment and initializing the Q-Networks.\n",
    "\n",
    "- ReplayBuffer is used for the experience replay mechanism critical for stabilizing training.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a830b8c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "\n",
    "from qnet import Qnet # Assuming qnet module is provided and includes the Qnet class and ReplayBuffer class\n",
    "from train import train # Assuming train module is provided and includes the training loop procedure\n",
    "\n",
    "def main():\n",
    "    env = gym.make('CartPole-v1')\n",
    "    q = Qnet()\n",
    "    q_target = Qnet()\n",
    "    q_target.load_state_dict(q.state_dict())\n",
    "    memory = ReplayBuffer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72310b6f",
   "metadata": {},
   "source": [
    "## Optimization Setup\n",
    "\n",
    "- The Adam optimizer is used for adjusting network weights.\n",
    "\n",
    "- Epsilon is the exploration rate, which we'll linearly anneal from 8% to 1%.\n",
    "\n",
    "- The main loop performs interactions with the environment and updates the Q-network.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aefdee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "    learning_rate = 0.0004\n",
    "    optimizer = optim.Adam(q.parameters(), lr=learning_rate)\n",
    "    print_interval = 20\n",
    "    score = 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76df4a00",
   "metadata": {},
   "source": [
    "## Main Training Loop\n",
    "\n",
    "- The loop iterates over episodes, selecting actions and updating the Q-network.\n",
    "\n",
    "- After every episode, it checks if the memory buffer is large enough to train the network.\n",
    "\n",
    "- The target network is updated at fixed intervals to stabilize learning.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a5dc94e",
   "metadata": {},
   "outputs": [],
   "source": [
    "    for n_epi in range(10000):\n",
    "        epsilon = max(0.01, 0.08 - 0.01*(n_epi/200)) # Linear annealing from 8% to 1%\n",
    "        s, _ = env.reset()\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            a = q.sample_action(torch.from_numpy(s).float(), epsilon)      \n",
    "            s_prime, r, done, truncated, info = env.step(a)\n",
    "            done_mask = 0.0 if done else 1.0\n",
    "            memory.put((s,a,r/100.0,s_prime, done_mask))\n",
    "            s = s_prime\n",
    "\n",
    "            score += r\n",
    "            if done:\n",
    "                break\n",
    "            \n",
    "        if memory.size()>2000:\n",
    "            train(q, q_target, memory, optimizer)\n",
    "\n",
    "        if n_epi%print_interval==0 and n_epi!=0:\n",
    "            q_target.load_state_dict(q.state_dict())\n",
    "            print(\"n_episode :{}, score : {:.1f}, n_buffer : {}, eps : {:.1f}%\".format(\n",
    "                                                            n_epi, score/print_interval, memory.size(), epsilon*100))\n",
    "            score = 0.0\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67e9f7ad",
   "metadata": {},
   "source": [
    "## Conclusion and Cleanup\n",
    "\n",
    "- By the end of training, the DQN agent should have learned to balance the pole on the cart.\n",
    "\n",
    "- It's important to always close the environment after training to free up resources.\n",
    "\n",
    "- Improvements can include adjusting hyperparameters, or trying different reward structures and architectures.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4dee005",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc2b76d1",
   "metadata": {},
   "source": [
    "## Introduction to DQN\n",
    "\n",
    "- Deep Q-Networks (DQN) blend Q-Learning with deep neural networks to allow agents to learn directly from high-dimensional sensory inputs.\n",
    "\n",
    "- This approach was famously applied by DeepMind to learn playing Atari games from pixel inputs.\n",
    "\n",
    "- In this notebook, we will build a basic DQN model for a simple environment.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbfb9d89",
   "metadata": {},
   "source": [
    "## Setting up the Environment\n",
    "\n",
    "- We will use OpenAI Gym's CartPole environment as a testbed.\n",
    "\n",
    "- The goal is to balance a pole on a cart by moving the cart left or right.\n",
    "\n",
    "- It is a simple but effective environment for demonstrating the capabilities of a DQN agent.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae341491",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Create the environment\n",
    "env = gym.make('CartPole-v1')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a454092",
   "metadata": {},
   "source": [
    "## Defining the Neural Network for DQN\n",
    "\n",
    "- The neural network will serve as the function approximator for our Q-values.\n",
    "\n",
    "- It takes the environment's state as input and outputs the Q-value for each action.\n",
    "\n",
    "- We use PyTorch to define our network.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "062bedab",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(DQN, self).__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(input_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, output_size)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.network(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db59709b",
   "metadata": {},
   "source": [
    "## Setting up the Training Loop\n",
    "\n",
    "- Training involves iteratively playing the game, storing experiences, and updating the model based on those experiences.\n",
    "\n",
    "- We sample actions using an epsilon-greedy strategy to balance exploration and exploitation.\n",
    "\n",
    "- Let's set up the main components needed for training our DQN agent.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3566d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def epsilon_greedy(state, epsilon):\n",
    "    # Implement epsilon greedy action selection\n",
    "    if np.random.rand() < epsilon:\n",
    "        return env.action_space.sample()  # Explore\n",
    "    else:\n",
    "        with torch.no_grad():\n",
    "            return model(state).argmax().item()  # Exploit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a60b6c1d",
   "metadata": {},
   "source": [
    "## Training the DQN Agent\n",
    "\n",
    "- We will now define the training loop for our DQN agent.\n",
    "\n",
    "- This loop involves running episodes, selecting actions, storing transitions, and updating the model.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf38c2b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "epochs = 1000\n",
    "batch_size = 32\n",
    "gamma = 0.99\n",
    "epsilon_start = 1.0\n",
    "epsilon_end = 0.01\n",
    "epsilon_decay = 0.995\n",
    "learning_rate = 0.0001\n",
    "\n",
    "# Model, optimizer, and loss function\n",
    "model = DQN(env.observation_space.shape[0], 64, env.action_space.n)\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    state = env.reset()\n",
    "    state = torch.FloatTensor(state).unsqueeze(0)\n",
    "    total_reward = 0\n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        epsilon = max(epsilon_end, epsilon_start * (epsilon_decay ** epoch))\n",
    "        action = epsilon_greedy(state, epsilon)\n",
    "        \n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        next_state = torch.FloatTensor(next_state).unsqueeze(0)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            next_q_values = model(next_state)\n",
    "            max_next_q_value = next_q_values.max(1)[0]\n",
    "            target_q_value = reward + (1 - done) * gamma * max_next_q_value\n",
    "        \n",
    "        pred_q_value = model(state)[0][action]\n",
    "        \n",
    "        loss = loss_fn(pred_q_value, target_q_value)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    "        \n",
    "    if epoch % 100 == 0:\n",
    "        print(f\"Epoch: {epoch}, Total Reward: {total_reward}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd7bcf2f",
   "metadata": {},
   "source": [
    "## Evaluating the DQN Agent\n",
    "\n",
    "- After training, it's important to evaluate our agent's performance in the environment without exploration noise.\n",
    "\n",
    "- This will give us a clearer picture of how well our agent has learned to solve the task.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2baa166",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_rewards = []\n",
    "for _ in range(100):\n",
    "    state = env.reset()\n",
    "    state = torch.FloatTensor(state).unsqueeze(0)\n",
    "    total_reward = 0\n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        action = model(state).argmax().item()  # Choose best action\n",
    "        state, reward, done, _ = env.step(action)\n",
    "        state = torch.FloatTensor(state).unsqueeze(0)\n",
    "        total_reward += reward\n",
    "    total_rewards.append(total_reward)\n",
    "\n",
    "print(f\"Average Reward: {np.mean(total_rewards)}\")\n",
    "# j2 from 'macros.j2' import header\n",
    "# {{ header(\"DQN\", \"DQN\") }}\n",
    "\n",
    "# j2 from 'macros.j2' import header\n",
    "# {{ header(\"DQN\", \"DQN\") }}\n",
    "\n",
    "# j2 from 'macros.j2' import header\n",
    "# {{ header(\"test\", \"test\") }}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
